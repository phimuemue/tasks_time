\chapter{Benchmarks}
\label{chap:benchmarks}

While chapter \ref{chap:p3} will focus on more theoretical aspects of our problem, we first summarize in short how the program performs in practice. We therefore ran tests that considered intrees with a certain number of tasks.

\emph{Remark:} It may be the case that the program (and in particular, the data shown in the tables) changed since this thesis was finished.

\section{Canonical vs. non-canonical variant}
\label{sec:benchmark-canonical-vs-non-canonical}

First we compare how the program performs with resp. without the elimination of equivalent snapshots (see section \ref{sec:intro-first-glance-schedules}). We therefore computed the LEAF schedules for all non-trivial intrees with a certain number of tasks and measured the run times. Table \ref{tab:comparison-canonical-vs-non-canonical} shows the results (we did not measure up to milliseconds but only seconds since the results speak for themselves and need not be more accurate). This table was created using a Intel Core2 with 2.13GHz and 2Gb of RAM. For 13 tasks, we had to stop the variant that did not exclude equivalent snapshots because it needed too much memory. 

\begin{table}[th]
  \centering
  \begin{tabular}[ht]{lcccccc}
    Tasks                         & $\leq 8$    & 9 & 10 & 11 & 12 & 13 \\
    \hline
    Equivalent snapshots included & $\leq 0.2s$ & $\approx 0.5s$ & $3s$ & $15s$ & $80s$ & $>360s$ \\
    Equivalent snapshots excluded & $\leq 0.2s$ & $\approx 0.3s$ & $2s$ & $6s$ & $20s$ & $75s$
  \end{tabular}
  \caption{Run time comparison with vs. without elimination of equivalent snapshots.}
  \label{tab:comparison-canonical-vs-non-canonical}
\end{table}

Even more important is the memory consumption: While the variant \emph{with} canonical snapshots used roughly 30mb of memory to process all non-trivial intrees with 12 tasks, we needed over 550mb when we did \emph{not} exclude equivalent snapshots.

We observe that excluding equivalent snapshots results both in a remarkable speedup and in a considerably smaller memory footprint. For this reason, we quite quickly decided to focus on the variant that excludes equivalent snapshots. All following benchmarks are done with equivalent snapshots excluded.

\section{Keeping all intrees in memory}
\label{sec:benchmarks-all-intrees-in-memory}

We now research how much time it takes to compute optimal schedules for intrees with a certain size.

As a first benchmark, we computed optimal schedules for all non-trivial intrees (with a certain amount of tasks) ``in one run'', i.e. we generated all non-trivial intrees, kept them in main memory and computed the optimal schedule for each of these intrees. This technique has the advantage that intermediate results can be re-used and do not need to be recomputed over and over again. On the other hand, keeping that many intrees in RAM leads to enormous memory consumption and, thus, was only done for up to 15 intrees.

Table \ref{tab:time-benchmark} shows the results. This table was created on a reasonably modern machine (with an Intel Core i7). Peak memory consumption was measured with Valgrind for up to 13 tasks. According to the valgrind manual \cite{massifmanual}, these measurements are accurate within 1\%. For more than 13 tasks, Valgrind was too slow, so we went for a more simplistic approach: We simply observed the stats shown by htop. This means, that the memory consumption for 14 and 15 tasks is not \emph{that} accurate, but probably still accurate enough to get an impression of how many memory was needed.

Moreover, we carried out these tests with simple floating point numbers to represent probabilities and expected run times. We also implemented another feature to support fractions --- see section \ref{sec:benchmarks-myfloat-variations} for more details.

\begin{table}[ht]
  \centering
  \begin{tabular}[ht]{ccccc}
    Tasks & Intrees & Snapshots & Time & Memory \\
    \hline{}
    $\leq 9$ & $\leq$ 171 & $\leq$ 891 & $\leq$ 0.5s & $\leq$ 1451040b \\
    10 & 433 & 3004 & 1s & 4941576b \\
    11 & 1123 & 10143 & 4s & 16847304b \\
    12 & 2924 & 34065 & 6s & 57016592b \\
    13 & 7720 & 113492 & 26s & 193781984b \\
    14 & 20487 & 375088 & 2m10s & $\approx$ 410mb \\
    15 & 54838 & 1230391 & 7m & $\approx$ 1.5Gb \\
    
  \end{tabular}
  \caption{Time needed to compute optimal schedules for all non-trivial intrees with a certain number of tasks using a floating-point representation for probabilities and run times.}
  \label{tab:time-benchmark}
\end{table}

The memory consumption might look quite high at first glance, but if we look closer, it becomes clear that quite a lot of things have to be stored. We have to store each snapshot containing the current intree, the scheduled tasks, and its successors. Moreover, we need a ``pool'' of snapshots that is used to avoid over and over recomputing equivalent snapshots. Moreover, the statistics in table \ref{tab:time-benchmark} of course include the memory that is consumed by e.g. auxiliary data structures from the C++ container classes.

The main problem is -- of course -- that the number of non-trivial intrees drastically grows as the number of tasks increases (1,1,2,2,5,11,28,67,171,433,1123,\dots --- see \cite{oeisnumbernontrivialintrees}). For that reason, we continue benchmarking by computing optimal schedules for single intrees (or a small set of intrees).

\section{Grouping intrees and computing them one after another}
\label{sec:benchmarks-clustered-intrees}

The next benchmark we conducted was done for intrees with 15 or more tasks. First, we generated all (nontrivial) intrees with a certain number of tasks. Afterwards, we split the whole collection of intrees into smaller chunks containing a certain amount of intrees. We then processed these chunks one after another. 

The results for this setting are shown in table \ref{tab:benchmark-clustered-run-time}.

In this scenario, the measurements become more difficult to interpret. Especially, the number of snapshots and the amount of memory can not be directly compared to the values shown in section \ref{sec:benchmarks-all-intrees-in-memory}. For the time, it is also non-trivial. This is because we carry out some computations twice. While we can measure the time quite accurately, we can not do this for the memory consumption because Valgrind slowed down the program too much to finish in reasonable time. This is why we used \texttt{htop} to estimate the amount of memory used for 15 and 16 tasks. We did not exactly measure the memory consumption for 17 or more tasks, but it can be said that we experienced no problems on a machine with 8Gb RAM.

\begin{table}[ht]
  \centering
  \begin{tabular}[ht]{ccccc}
    Tasks & Intrees & Chunk size & Time & Memory per chunk \\
    \hline
    15 & 54838 & 5000 & 11m & $<$ 500mb \\
    16 & 147570 & 5000 & 1h & $<$ 800mb \\
    17 & 399466 & 5000 & 3h & n.a. \\
    18 & 1086312 & 6000 & 13.3h & n.a. \\
    19 & 2967517 & 6000 & 24h & n.a.
  \end{tabular}
  \caption{Time needed to compute optimal schedules for all non-trivial intrees  with a certain number using sets of subtrees of tasks using floating point numbers for probabilities and run times.}
  \label{tab:benchmark-clustered-run-time}
\end{table}

One interesting fact that can be observed from table \ref{tab:benchmark-clustered-run-time} is that for 15 tasks, we needed roughly 11 minutes, while in the non-grouped case, it took only about the half of this time (about 6 minutes --- see table \ref{tab:time-benchmark}). That is, as expected, grouping the intrees into smaller chunks considerably increases the run time, but -- on the other hand -- seems to be the only real alternative since the number of trees grows that fast so they do not fit into main memory anymore.

As you can see, the time needed to compute optimal schedules for all non-trivial intrees increases very fast with growing number of tasks. Of course, the run time can be reduced by parallelizing the computations for example in a way that computes different chunks in parallel.

\section{Other representations for probabilities and expected values}
\label{sec:benchmarks-myfloat-variations}

As said before, the user can choose between different representations for probailities and expected values. By default, we simply use C++'s \texttt{float}s but for some scenarios it might be useful to have precise fractions. We implemented those using Boost Rational Number library or GNU Multiple Precision Arithmetic Library (GMP).

We compare the performance of the program with different representations, namely the default \texttt{float} representation, the representation by Boost's Rational Number library with \texttt{unsigned long long}s as denominators and numerators, and representationn by GMP. The results are shown in table \ref{tab:comparison-myfloat-variants} (generated on an Intel Core2 with 2.13GHz).

\begin{table}[th]
  \centering
  \begin{tabular}[ht]{lccccc}
    Tasks & 9 & 10 & 11 & 12 & 13 \\
    \hline 
    \texttt{float} & $\leq 0.3s$ & $2s$ & $6s$ & $20s$ & $75s$ \\
    Boost Raionals & $\leq 0.3$ & $2s$ & $6s$ & $21s$ & $78s$ \\
    GMP & $\leq 0.6s$ & $3s$ & $10s$ & $38s$ & $140s$
  \end{tabular}
  \caption{Comparison of different representations for probabilities and expected values.}
  \label{tab:comparison-myfloat-variants}
\end{table}

We see that Boost's Rational Number library does not considerably increase run time, while GMP almost doubles it. This may come from the fact that GMP uses ``arbitrarily large'' integers as denominators and numerators to represent fractions, thus allowing arbitrary precision (within memory limits), while we can adjust which underlying representation for numerators and denominators Boost should use. Since we are using common \texttt{unsigned long long}s, the operations on these are no very expensive, while the GMP counterparts might be due to overflow checks, dynamic memory allocation for larger numbers, etc. However, \emph{only} GMP offers -- out of the box -- arbitrary precision, while Boost's Rational Number library might yield wrong results if the denominators and numerators get too big for \texttt{unsigned long long}s.

Surprisingly, using Boost's Rationals did not increase memory consumption considerably. On the other hand, GMP also increased the memory consumption by roughly 40\%. This again comes possibly from the fact that GMP offers arbitrary precision, thereby requiring to store arbitrarily large numbers and requiring auxiliary data structures.

Finally, we observer that using rational numbers does not increase run time as much as not eliminating equivalent snapshots, but the additional amount of time with GMP is considerably large -- as a tradeoff we get arbitrary precision.

\emph{Remark:} We almost never ran into problems when we used simple \texttt{float}s to represent probabilities. Only in rare cases we had to rely on GMP because of rounding errors that possibly gave us incorrect results. For this reason, we decided that the default representation shall use \texttt{float}s.

%%% Local Variables:
%%% TeX-master: "../thesis.tex"
%%% End: 