\chapter{Suboptimal strategies for P3}
\label{chap:p3-suboptimal}

In this section, we take a look at some strategies and use them for scheduling with three processors. We will do this very exemplarily and inspect counterexamples for the respective strategies. In particular we will see examples illustrating that HLF does not work in all cases. All the examples for the devised strategies are minimal in the sense that there are no intrees containing fewer tasks that could serve as a counter example for the respective strategy.
Finally, we conclude some properties that \emph{optimal} scheduling strategies seem to fulfill.

\section{HLF}
\label{sec:hlf-p3-suboptimal}

First of all, we take look at some examples that show that HLF is not optimal for the three processor case. We will consider several phenomena that can occur if we use HLF with three processors.

\subsection{HLF does not behave the same for intrees with same profile}
\label{sec:p3-suboptimal-hlf-same-profiles-different-run-times}

In the two-processor case it is known that trees with the same level profile (see section \ref{sec:p2-profiles}) have the same run time. This is not the case for three processors. Figure \ref{fig:hlf-001112} shows an intree, where HLF can choose at some points, and different choices result in different runtimes.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/001112_hlf_subopt.pdf}
    \caption{Suboptimal HLF run}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/001112_hlf_opt.pdf}
    \caption{Optimal HLF run. This is also the overal optimal schedule.}
    \label{fig:hlf-001112-optimal-version}
  \end{subfigure}
  \caption{HLF on $(0,0,1,1,1,2)$. Different runs of HLF do not necessarily produce the same result.}
  \label{fig:hlf-001112}
\end{figure}

Because HLF can produce different run times depending which task it has chosen, it is clear that HLF in its raw form can not be optimal. The following section reveals even more.

\subsection{Examples where HLF is strictly suboptimal}
\label{sec:p3-suboptimal-hlf-strictly-suboptimal}

The example from figure \ref{fig:hlf-001112-optimal-version} shows the optimal run. We observe that this run is a specific instance of HLF, because at each point of time, always tasks with the highest level numbers are chosen.

However, there are intrees, where \emph{no} HLF-run is optimal. Figures \ref{fig:hlf-vs-opt-0012346688}, \ref{fig:hlf-vs-opt-0012446788} and \ref{fig:hlf-vs-opt-00123455799} show some examples for which this is exactly the case.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/0012346688_subopt.pdf}
    \caption{HLF -- suboptimal}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/0012346688_opt.pdf}
    \caption{Optimal run is non-HLF}
  \end{subfigure}
  \caption{HLF vs. optimal solution for $(0,0,1,2,3,4,6,6,8,8)$}
  \label{fig:hlf-vs-opt-0012346688}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/0012446788_subopt.pdf}
    \caption{HLF -- suboptimal}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/0012446788_opt.pdf}
    \caption{Optimal run is non-HLF}
  \end{subfigure}
  \caption{HLF vs. optimal solution for $(0,0,1,2,4,4,6,7,8,8)$ (taken from Ernst Mayr)}
  \label{fig:hlf-vs-opt-0012446788}
\end{figure}
\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/00123455799_subopt.pdf}
    \caption{HLF -- suboptimal}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/hlf_not_optimal/00123455799_opt.pdf}
    \caption{Optimal run is non-HLF}
  \end{subfigure}
  \caption{HLF vs. optimal solution for $(0,0,1,2,3,4,5,5,7,9,9)$ (taken from Chandy/Reynolds)}
  \label{fig:hlf-vs-opt-00123455799}
\end{figure}

\subsection{Quality of HLF}
\label{sec:suboptimal-hlf-quality}

We have seen that HLF is suboptimal in some cases, but experience shows that HLF is quite good in many cases. In fact \cite{journals/siamcomp/PapadimitriouT87} have shown that the following theorem holds:

\begin{theorem}
  There is a function $\beta: \mathbb{N} \mapsto \mathbb{R}^+_0$ with $\lim_{n\rightarrow \infty} \beta(n) = 0$ such that for each intree $I$ and an arbitrary HLF strategy $HLF$ we have
  \begin{equation*}
    T_{HLF}(I) \leq \inf_\pi\, T_{\pi}(I) \cdot \left( 1+\beta(N) \right),
  \end{equation*}
  where $T_{P}(I)$ denotes the expected run time for intree $I$ if it is scheduled by policy $P$, $N$ is the number of tasks in $I$ and the infimum is taken over all scheduling strategies $\pi$.
\end{theorem}

\begin{proof}
  See \cite{journals/siamcomp/PapadimitriouT87}.
\end{proof}

This result is quite useful because it shows that HLF is -- even if not optimal -- quite close as the number of tasks grows.

\subsection{Different categories of sub-optimality}
\label{sec:hlf-suboptimal-two-variants}

As we saw, there are two possibilities for the sub-optimality of HLF:

\begin{itemize}
\item \emph{Not each possible run} of HLF yields the same run time.
\item The optimal run has to choose a strict non-HLF task.
\end{itemize}

We use the following nomenclature: A strategy is called \emph{can-optimal} (a term already used in \cite{MoritzMaasDiploma}), if it \emph{might} result in an optimal schedule. A strategy that \emph{can not} produce an optimal solution is called \emph{strictly suboptimal} (or simply suboptimmal if it is clear from the context). 

It is a notable fact that there are many cases, where HLF is can-optimal. 

We use this distinction to (roughly) differentiate the following strategies into two categories: 

\begin{itemize}
\item The cases where HLF is strictly suboptimal lead us to several strategies that are presented in section \ref{sec:suboptimal-non-hlf-strategies}. These strategies are strict counterparts to HLF.
\item For the cases where HLF is can-optimal, we examined several strategies that try to determine which tasks should be chosen to minimize the expected run time. These strategies are presented in section \ref{sec:suboptimal-hlf-can-optimal-strategies}. These strategies can be seen as ``refinements'' of HLF such that HLF behaves better.
\end{itemize}

We will not only focus on particular strategies, but we will focus on which snapshots can be excluded or which particular structure snapshots might have. That is, the strategies we consider are in many cases ambiguous because they admit several possible choices. However, they do \emph{not} allow \emph{all} possible choices, thereby possibly reducing the amount of snapshots to examine. Not all strategies fall strictly into one of the groups -- we then put it into the category we found more insightful.

\section{(Dynamic) list scheduling}
\label{sec:suboptimal-strategies-list-scheduling}

List scheduling considers the current intree and generates a list of tasks sorted in such a way that the tasks that come first in the list shall be priorized and scheduled first, if possible. As shown by \cite{MoritzMaasDiploma}, dynamic list scheduling strategies can not be optimal for non-preemtive scheduling of intrees (whose tasks' run times are exponentially distributed) with three processors.

This can be seen by examining the intree $(0,0,1,1,1,2,2,2,2,3,6)$ as shown in figure \ref{fig:list-scheduling-counter-example}. Because the subtree shown in figure \ref{fig:list-schedule-counter-example-subtree} is optimally scheduled using other tasks than the one that have to be used when it is reached in the original schedule, there can not exist a dynamic list scheduling strategy that works for \emph{all} intrees.

Note that this also excludes HLF from being optimal.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{.45\textwidth}
    \centering
    \includegraphics{p3/list_sched_0011122236_opt_sched.pdf}
    \caption{Optimal schedule for intree $(0,0,1,1,1,2,2,2,2,3,6)$.}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{.45\textwidth}
    \centering
    \vfill
    \includegraphics{p3/list_sched_subtree.pdf}
    \vfill
    \caption{This subtree of $(0,0,1,1,1,2,2,2,3,6)$ is optimally scheduled as shown, but in the optimal schedule for the original intree, it has to be scheduled in another way.}
    \label{fig:list-schedule-counter-example-subtree}
  \end{subfigure}
  \caption{A counterexample for list scheduling}
  \label{fig:list-scheduling-counter-example}
\end{figure}

That is, an optimal strategy has to consider which tasks are already scheduled from previous steps and can not rely \emph{only} on the current intree's structure. We experience this phenomenon in many intrees shown in this chapter. However, for the initial snapshot, we -- of course -- must rely on the structure of the intree. Thus, we will examine some strategies and inspect if the first snapshot of an optimal schedule adheres specific rules.

\section{Non-HLF strategies}
\label{sec:suboptimal-non-hlf-strategies}

We saw in section \ref{sec:hlf-p3-suboptimal} that there are examples where HLF is strictly suboptimal. We now present some strategies that we took into consideration and that we examined w.r.t. their optimality. 

We therefore considered optimal schedules and examined whether the initial snapshot (i.e. the initial choice of tasks) are formed upon a certain pattern.

These strategies are strongly different from HLF and rely upon the structure of the intree. None of the strategies considered lead us to strictly optimal results in all cases.

\subsection{``2-HLF plus 1''}
\label{sec:disproving-2hlf-plus-1}

We examined all intrees with up to 13 tasks, especially the cases where HLF is not optimal. Thereby, we obsered that in all cases where three tasks could be scheduled, the optimal solution scheduled two tasks, that could be chosen by HLF for two processors and only the third task \emph{might} be a task that would not have been chosen by HLF (see figures \ref{fig:hlf-vs-opt-0012346688}, \ref{fig:hlf-vs-opt-0012446788} and \ref{fig:hlf-vs-opt-00123455799} as particular instances of those). Thus, we examined whether an optimal scheduling strategy for three processors has always \emph{at most one} task that is non-HLF. Interestingly, there is an intree with 14 tasks, whose optimal schedule starts out by choosing the single topmost task and \emph{two} non-HLF tasks. This intree ($(0,0,1,2,2,3,3,6,8,9,10,11,12)$) is shown in figure \ref{fig:2-hlf-plus-one-not-optimal}. We can generalize this intree to a whole family of intrees where the optimal strategy initially chooses the single topmost task, and two lowest-level leaves by adding leaves along the longest chain. This results in intrees of the form $(0, 0, 1, 2, 3, 4, 4, 5, 5, 8, 10, 11, 12, 13, 14, 15,\dots)$.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics{p3/2hlf_suboptimal/0012233689101112_opt.pdf}
    \caption{Optimal schedule picking \emph{two} non-HLF tasks.}
  \end{subfigure}
  \begin{subfigure}{.58\textwidth}
    \centering
    \includegraphics{p3/2hlf_suboptimal/0012233689101112_subopt.pdf}
    \caption{HLF-Schedule}
  \end{subfigure}
  \caption{Intree $(0,0,1,2,2,3,3,6,8,9,10,11,12)$ requires the optimal schedule to start out by choosing two non-HLF tasks.}
  \label{fig:2-hlf-plus-one-not-optimal}
\end{figure}

\subsection{Only highest or lowest leaves}
\label{sec:disproving-only-highest-or-lowest-leaves}

The trees we examined so far resulted in schedules that picked only combinations \emph{highest leaves and lowest leaves} possible. Thus, we were tempted to think that an optimal schedule chooses only topmost tasks or leaves whose level is minimal (among all leaves). However, this is not a criterion for an optimal schedule, as we can observe by scheduling the 13-tasks-intree $(0,0,0,2,3,4,5,7,7,9,10,10)$, which is shown in figure \ref{fig:only-high-or-low-not-optimal}. For this intree, we have to schedule two topmost tasks and one task on level 3, but there is one unscheduled task remaining on level 1.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics{p3/only_high_or_low/00023457791010_opt.pdf}
    \caption{Optimal schedule picking a non-HLF task that is also not the lowest possible.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics{p3/only_high_or_low/00023457791010_subopt.pdf}
    \caption{Suboptimal HLF-Schedule.}
  \end{subfigure}
  \caption{Intree $(0,0,0,2,3,4,5,7,7,9,10,10)$ shows that there are intrees where an optimal schedule has to choose a non-HLF task that has a higher level than some non-chosen task.}
  \label{fig:only-high-or-low-not-optimal}
\end{figure}

\section{Refining can-optimal HLF}
\label{sec:suboptimal-hlf-can-optimal-strategies}

As mentioned in section \ref{sec:hlf-suboptimal-two-variants}, there are many cases where HLF is \emph{can-optimal}, i.e. where the optimal schedule always has tasks of the highest levels scheduled, but not each HLF schedule is optimal. This results from situations where HLF can choose one from several task as the next task to be scheduled. We describe some strategies that try to eliminate these ambiguities and give counterexamples that show that these strategies are not optimal.

\subsection{``As few free paths as possible''}
\label{sec:disproving-hlf-no-free-chain}

(For now,) we call a path from the root to a leaf (i.e. a ready task) $t$ \emph{free} if $t$ is not scheduled.

One might be tempted to think that it should be the foremost goal to exploit parallelism as good as possible and that this might be acchieved by choosing the currently scheduled tasks in a manner such that as few free paths as possible in an optimal schedule. That is, we choose the leaves in a way so that the ends of as many different paths as possible are scheduled. This strategy was the first that came to our mind and was inspired by looking at the counterexamples against HLF depicted in figures \ref{fig:hlf-001112}, \ref{fig:hlf-vs-opt-0012346688}, \ref{fig:hlf-vs-opt-0012446788} and \ref{fig:hlf-vs-opt-00123455799}. We observe for these intrees that the optimal schedules has no as few free chains as possible.

However, there are examples where the optimal contains snapshots that do not adhere this property. Consider e.g. the tree $(0,0,0,1,1,1,2,2,3)$ (figure \ref{fig:hlfnfc-is-not-optimal} compares the optimal schedule to the no-free-paths schedule).

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics{p3/hlfnfc_not_optimal/000111223_hlfnfc.pdf}
    \caption{HLF schedule while choosing tasks such that there are as few free paths as possible -- overall run time of 5.20199.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics{p3/hlfnfc_not_optimal/000111223_opt.pdf}
    \caption{Optimal schedule (run time 5.20028) has three free paths at the beginning.}
  \end{subfigure}
  \caption{HLF with as few free paths as possible is not necessarily optimal.}
  \label{fig:hlfnfc-is-not-optimal}
\end{figure}

\subsection{Subtree with minimum number of topmost tasks}
\label{sec:suboptimal-hlf-can-optimal-subtree-fewest-toptasks}

If we consider the intree $(0,0,0,1,1,1,2,2,3)$, we have seen that an optimal schedule picks 7,8 and 9 as initially scheduled tasks (see figure \ref{fig:hlfnfc-is-not-optimal}). Moreover, in many cases, topmost-tasks that are the \emph{single direct predecessor} of their respective direct successor, are chosen by an optimal schedule.

These facts lead us to the suspicion that -- if we have an intree for which HLF is can-optimal -- (informally) we should pick the subtrees with the lowest number of topmost tasks. 

In this context, it is important to exactly describe which subtree we are talking about. Therefore, we employ the following definition:

\begin{definition}[Toptask-maximal subtree for a leaf]
  Let $t$ be a leaf of an intree $I$ and let $p=(t, t_1, t_2, t_3, \dots, r)$ be the path from $t$ to the root $r$.

  The \emph{toptask-maximal subtree for a leaf} $t$ is the subtree rooted at the \emph{lowest} task $t^*$ within $p$ that is not $t$ and that does \emph{not} contain more topmost tasks than the subtree rooted at the predecessor of $t^*$ within $p$.
\end{definition}

As an example, consider the following intree (remember that topmost tasks are defined to be the tasks whose levels are at least as large as the level of \emph{any} task in the intree --- see section \ref{sec:foundations-graph-theory}):

\begin{center}
  \begin{tikzpicture}[scale=.6, anchor=south]
    \node[circle, scale=0.9, draw] (tid0) at (3,1.5){0};
    \node[circle, scale=0.9, draw] (tid1) at (2.25,3){1};
    \node[circle, scale=0.9, draw] (tid2) at (1.5,4.5){3};
    \node[circle, scale=0.9, draw] (tid7) at (0.15,6){6};
    \node[circle, scale=0.9, draw] (tid10) at (1.5,6){7};
    \draw[ thick](tid2) -- (tid7);
    \draw[ thick](tid2) -- (tid10);
    \node[circle, scale=0.9, draw] (tid3) at (3.9,4.5){4};
    \node[circle, scale=0.9, draw] (tid5) at (2.85,6){8};
    \node[circle, scale=0.9, draw] (tid6) at (2.85,7.5){\small 10};
    \draw[ thick](tid5) -- (tid6);
    \draw[ thick](tid2) -- (tid5);
    \draw[ thick](tid1) -- (tid2);
    \draw[ thick](tid1) -- (tid3);
    \node[circle, scale=0.9, draw] (tid4) at (5.25,3){2};
    \node[circle, scale=0.9, draw] (tid9) at (4.3,6){9};
    \draw[ thick](tid3) -- (tid9);
    \node[circle, scale=0.9, draw] (tid11) at (4.3,7.5){11};
    \draw[ thick](tid11) -- (tid9);
    \node[circle, scale=0.9, draw] (tid8) at (5.25,4.5){5};
    \draw[ thick](tid4) -- (tid8);
    \draw[ thick](tid0) -- (tid1);
    \draw[ thick](tid0) -- (tid4);
  \end{tikzpicture}
\end{center}

The maximal subtree for leaf 10 is the subtree rooted at node 3, which can be derived as follows:
\begin{itemize}
\item The path from 10 to the root 0 is given by $p=(10,8,3,1,0)$.
\item We consider the subtrees rooted at the tasks along this path, and denote the subtree rooted at node $x$ by $I_x$:
  \begin{itemize}
  \item The subtree rooted at 10 (called $I_{10}$) contains only the topmost task 10.
  \item Subtree $I_8$ contains only topmost task 10.
  \item Subtree $I_3$ still contains only 10 as the topmost task (it introduces only a new leaf, namely 7).
  \item Subtree $I_1$ contains 10 \emph{and} 11 as topmost tasks.
  \end{itemize}
\item As seen, task 3 is the lowest task within $p$ that does not contain more topmost tasks than its predecessor (in the path from the leaf to the root).
\end{itemize}

A strategy for cases where HLF is can-optimal now might be to resolve HLF-ambiguities as follows:
\begin{itemize}
\item Generate all possible choices that could result from HLF.
\item For each topmost task, compute the topmost-maximal subtree.
\item Prefer topmost tasks whose topmost-maximal subtrees contain fewer topmost tasks.
\end{itemize}

The last step in the above explanation can be viewed as follows: We first create all possible choices of topmost tasks and then only pursue those, where there is no topmost-maximal subtree that has unscheduled topmost tasks.

This strategy seems to do a good job in many cases, but can be seen to be false by examining the optimal schedule for the following intree with 18 tasks: $(0,0,1,2,2,2,3,3,3,4)$. It is shown in figure \ref{fig:subtree-with-fewest-toptasks-suboptimal}.

\emph{Remark:} We did not specify what should be done if there are several maximal subtrees with the same number of topmost tasks, but our counterexample suffices that this strategy does not work optimally even if there are no maximal subtrees with the same number of nodes.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \input{p3/subtree_with_fewest_toptasks/subtree_with_fewest_toptasks_subopt}
    \caption{If we initially start with tasks as shown, this is the best schedule that can be obtained.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\textwidth}
    \centering
    \input{p3/subtree_with_fewest_toptasks/subtree_with_fewest_toptasks_opt}
    \caption{Optimal schedule picking a subtree with three topmost tasks.}
  \end{subfigure}
\caption{Intree $(0,0,1,2,2,2,3,3,3,4)$: For this intree, the optimal schedule chooses all tasks from a subtree with three topmost tasks and chooses none of the subtree with only one topmost tasks.}
  \label{fig:subtree-with-fewest-toptasks-suboptimal}
\end{figure}

\subsection{Subtree with maximum or minimum number of leaves}
\label{sec:suboptimal-hlf-can-optimal-subtree-fewest-leaves}

It can be quickly seen that slightly altering the strategy given in section \ref{sec:suboptimal-hlf-can-optimal-subtree-fewest-toptasks} in the sense that we do not concentrate on the number of \emph{topmost tasks} in a maximal subtree, but more generraly on the number of \emph{leaves} in a maximal subtree, does not yield a successful strategy. We adapt the notion of topmost-maximal subtrees in a straightforward manner:

\begin{definition}[Leaf-maximal subtree for a leaf]
  Let $t$ be a leaf of an intree $I$ and let $p=(t, t_1, t_2, t_3, \dots, r)$ be the path from $t$ to the root $r$.

  The \emph{leaf-maximal subtree for a leaf} $t$ is the subtree rooted at the \emph{lowest} task $t^*$ within $p$ that is not $t$ and that does \emph{not} contain more leaves than the predecessor of $t^*$ within $p$.
\end{definition}

\begin{description}
\item [Preferring leaf-maximal subtrees with fewer leaves] Figure \ref{fig:subtree-with-fewest-toptasks-suboptimal} shows that this strategy is not optimal, since the optimal solution prefers a subtree with four leaves over one with only three leaves.
  \emph{Remark:} The tree $(0,0,1,2,2,2,3,3,3,4)$ in particular shows that there are situations where a topmost task that is the \emph{single} requirement for its successor is initially \emph{not} scheduled in the optimal case. During our research we have experienced that this is a very rare situation.
\item [Preferring leaf-maximal subtrees with more leaves] One of our first examples, the intree $(0,0,0,1,1,1,2,2,3)$ (see figure \ref{fig:hlfnfc-is-not-optimal}) already shows that this strategy is not optimal in general.
\end{description}

\subsection{Preferring root's predecessors with longest processing time}
\label{sec:suboptimal-hlf-can-roots-longest-predecessors}

We also tried a recursive approach that decomposed an intree as follows: We separate the intrees rooted at the predecessors of the root. This way, we get a whole set of intrees. For each subtree, we now compute the optimal schedule and, moreover, the expected processing time -- assuming three processors in each individual subtree. The schedule for the whole intree then shall prefer subtrees whose expected processing time is the longest.

We were tempted to conjecture this because of the intree $(0,0,1,1,2,3,3,4,5,7,8,9,9,11,13,13,13)$ whose optimal schedule starts shown in figure \ref{fig:subtree-with-fewest-toptasks-suboptimal}. If we decompose this intree into its parts, we see that the subtree whose expected run time is maximal is the one whose tasks are initially scheduled in the optimal schedule (see figure \ref{fig:reasoning-for-longest-root-subtree}).

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \begin{tikzpicture}[scale=.2, anchor=south]
      \node[circle, scale=0.75, fill] (tid0) at (5.25,1.5){};
      \node[circle, scale=0.75, fill] (tid1) at (2.25,3){};
      \node[circle, scale=0.75, fill] (tid3) at (1.5,4.5){};
      \node[circle, scale=0.75, fill] (tid6) at (0.75,6){};
      \node[circle, scale=0.75, fill] (tid7) at (2.25,6){};
      \node[circle, scale=0.75, fill] (tid10) at (2.25,7.5){};
      \draw[](tid7) -- (tid10);
      \draw[](tid3) -- (tid6);
      \draw[](tid3) -- (tid7);
      \node[circle, scale=0.75, fill] (tid4) at (3.75,4.5){};
      \node[circle, scale=0.75, fill] (tid8) at (3.75,6){};
      \node[circle, scale=0.75, fill] (tid11) at (3.75,7.5){};
      \node[circle, scale=0.75, fill] (tid14) at (3.75,9){};
      \draw[](tid11) -- (tid14);
      \draw[](tid8) -- (tid11);
      \draw[](tid4) -- (tid8);
      \draw[](tid1) -- (tid3);
      \draw[](tid1) -- (tid4);
      \node[circle, scale=0.75, fill] (tid2) at (7.5,3){};
      \node[circle, scale=0.75, fill] (tid5) at (7.5,4.5){};
      \node[circle, scale=0.75, fill] (tid9) at (7.5,6){};
      \node[circle, scale=0.75, fill] (tid12) at (5.25,7.5){};
      \node[circle, scale=0.75, fill] (tid13) at (8.25,7.5){};
      \node[circle, scale=0.75, fill, task_scheduled] (tid15) at (6.75,9){};
      \node[circle, scale=0.75, fill, task_scheduled] (tid16) at (8.25,9){};
      \node[circle, scale=0.75, fill, task_scheduled] (tid17) at (9.75,9){};
      \draw[](tid13) -- (tid15);
      \draw[](tid13) -- (tid16);
      \draw[](tid13) -- (tid17);
      \draw[](tid9) -- (tid12);
      \draw[](tid9) -- (tid13);
      \draw[](tid5) -- (tid9);
      \draw[](tid2) -- (tid5);
      \draw[](tid0) -- (tid1);
      \draw[](tid0) -- (tid2);
    \end{tikzpicture}  
    \caption{Intree with initially scheduled tasks (for optimal schedule).}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\textwidth}
    \centering
    \begin{tikzpicture}[scale=.2, anchor=south]
      \node[circle, scale=0.75, fill] (tid1) at (2.25,3){};
      \node[circle, scale=0.75, fill] (tid3) at (1.5,4.5){};
      \node[circle, scale=0.75, fill] (tid6) at (0.75,6){};
      \node[circle, scale=0.75, fill] (tid7) at (2.25,6){};
      \node[circle, scale=0.75, fill] (tid10) at (2.25,7.5){};
      \draw[](tid7) -- (tid10);
      \draw[](tid3) -- (tid6);
      \draw[](tid3) -- (tid7);
      \node[circle, scale=0.75, fill] (tid4) at (3.75,4.5){};
      \node[circle, scale=0.75, fill] (tid8) at (3.75,6){};
      \node[circle, scale=0.75, fill] (tid11) at (3.75,7.5){};
      \node[circle, scale=0.75, fill] (tid14) at (3.75,9){};
      \draw[](tid11) -- (tid14);
      \draw[](tid8) -- (tid11);
      \draw[](tid4) -- (tid8);
      \draw[](tid1) -- (tid3);
      \draw[](tid1) -- (tid4);
      \node at (2.25, 0){5.67991};
      \begin{scope}[xshift=4cm]
        \node[circle, scale=0.75, fill] (tid2) at (7.5,3){};
        \node[circle, scale=0.75, fill] (tid5) at (7.5,4.5){};
        \node[circle, scale=0.75, fill] (tid9) at (7.5,6){};
        \node[circle, scale=0.75, fill] (tid12) at (5.25,7.5){};
        \node[circle, scale=0.75, fill] (tid13) at (8.25,7.5){};
        \node[circle, scale=0.75, fill] (tid15) at (6.75,9){};
        \node[circle, scale=0.75, fill] (tid16) at (8.25,9){};
        \node[circle, scale=0.75, fill] (tid17) at (9.75,9){};
        \draw[](tid13) -- (tid15);
        \draw[](tid13) -- (tid16);
        \draw[](tid13) -- (tid17);
        \draw[](tid9) -- (tid12);
        \draw[](tid9) -- (tid13);
        \draw[](tid5) -- (tid9);
        \draw[](tid2) -- (tid5);
        \node at (7.5, 0){6.83333};
      \end{scope}
    \end{tikzpicture}  
    \caption{Removing the root yields two subtrees with optimal expected runtimes (for three processors) as noted.}
  \end{subfigure}
  \caption{Intree $(0,0,1,1,2,3,3,4,5,7,8,9,9,11,13,13,13)$ and its corresponding subtrees rooted at the root's predecessors}
  \label{fig:reasoning-for-longest-root-subtree}
\end{figure}

Once again, this strategy can be shown to be suboptimal by considering $(0,0,0,1,1,1,2,2,3)$ (as depicted in figure \ref{fig:hlfnfc-is-not-optimal}) whose root has the following three predecessor intrees.

\begin{center}
  \begin{tikzpicture}[scale=.3]
    \begin{scope}
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \node[circle, fill, scale=.5] (1) at (-1,1){};
      \node[circle, fill, scale=.5] (2) at (0,1){};
      \node[circle, fill, scale=.5] (3) at (1,1){};
      \draw(0)--(1);
      \draw(0)--(2);
      \draw(0)--(3);
      \node at (0, -1.5){2.83333};
    \end{scope}
    \begin{scope}[xshift=5cm]
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \node[circle, fill, scale=.5] (1) at (-.5,1){};
      \node[circle, fill, scale=.5] (3) at (.5,1){};
      \draw(0)--(1);
      \draw(0)--(3);
      \node at (0, -1.5){2.5};
    \end{scope}
    \begin{scope}[xshift=10cm]
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \node[circle, fill, scale=.5] (1) at (0,1){};
      \draw(0)--(1);
      \node at (0, -1.5){2};
    \end{scope}
  \end{tikzpicture}
\end{center}

For $(0,0,0,1,1,1,2,2,3)$ the optimal schedule initially chooses the tasks 7,8 and 9 (the respective subtrees have run times 2.5 and 2).

\subsection{Preferring root's predecessors with shortest processing time}
\label{sec:suboptimal-preferring-root-predecessors-shortest-time}

It is clear that the opposite of the strategy from section \ref{sec:suboptimal-hlf-can-roots-longest-predecessors}, namely preferring those subtrees whose processing time is shortest, does also not yield correct results. This can be easily seen by considering the intree $(0,1,2,3,0,5,6,0,8,0)$ that is optimally scheduled by HLF (see section \ref{sec:p3-parallel-chains} for a proof) and whose root has the three following predecessors:

\begin{center}
  \begin{tikzpicture}[scale=.3]
    \begin{scope}[xshift=15cm]
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \node[circle, fill, scale=.5] (1) at (0,1){};
      \node[circle, fill, scale=.5] (2) at (0,2){};
      \node[circle, fill, scale=.5] (3) at (0,3){};
      \draw(0)--(3);
      \node at (0, -1.5){4};
    \end{scope}
    \begin{scope}[xshift=20cm]
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \node[circle, fill, scale=.5] (1) at (0,1){};
      \node[circle, fill, scale=.5] (2) at (0,2){};
      \draw(0)--(2);
      \node at (0, -1.5){3};
    \end{scope}
    \begin{scope}[xshift=25cm]
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \node[circle, fill, scale=.5] (1) at (0,1){};
      \draw(0)--(1);
      \node at (0, -1.5){2};
    \end{scope}
    \begin{scope}[xshift=30cm]
      \node[circle, fill, scale=.5] (0) at (0,0){};
      \draw(0)--(0);
      \node at (0, -1.5){1};
    \end{scope}
  \end{tikzpicture}
\end{center}

\subsection{``Filling up subtrees''}
\label{sec:suboptimal-filling-subtrees}

We observed that (for three processors) an optimal schedule looks as if it chose certain subtrees and ``filled them up one after another''. This is probably most concise formalized as the following pattern:

\begin{itemize}
\item Identify disjoint leaf-maximal (or topmost-maximal) subtrees of the whole intree.
\item Assign priorities to the subtrees so they are sorted according to this priority (thus, we have a sequence of subtrees $S_1,\dots,S_r$).
\item Schedule as many tasks from $S_1$ as possible. If all ready tasks in $S_1$ are scheduled, schedule as many tasks as possible in $S_2$. If all tasks in $S_2$ are scheduled, schedule as many tasks as possible in $S_3$.
\end{itemize}
%In an optimal schedule, we can -- without loss of generality -- assume that it is \emph{not} the case that there are two distinct scheduled tasks $x$ and $y$ with different successors and each of them having a non-scheduled, but ready task.

An alternative formulation of the above strategy states that there is at most one task having both scheduled and non-scheduled predecessors that are leaves. 

We have already seen that there are optimal schedules violating this property, e.g. the intree $(0,0,0,1,1,2,2,3)$. But for those intrees, there is another schedule having \emph{exact the same} run time but fulfilling the property.

The example intree $(0,0,0,1,1,2,2,3)$ admits two different schedules (one beginning with tasks 4,5,8 and the other beginning with 4,6,8) that have exactly the same run time (see figure \ref{fig:filling-up-without-loss-of-generality}). The reason is that both schedules result in equivalent snapshots with the same probabilities after the first task finishes.

\begin{figure}[th]
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics{p3/suboptimal/000111223_opt458.pdf}
    \caption{Optimal schedule starting with tasks 4,5 and 8.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics{p3/suboptimal/000111223_opt468.pdf}
    \caption{Optimal schedule starting with tasks 4,6 and 8.}
  \end{subfigure}
  \caption{The intree $(0,0,0,1,1,2,2,3)$ has two different schedules reaching the optimum expected run time.}
  \label{fig:filling-up-without-loss-of-generality}
\end{figure}

For trees with fewer than 12 tasks, we could not find any tree that violated our conjecture, but the intree $(0,0,0,2,2,3,5,5,6,6,6)$ has the interesting property that the optimal schedule for this intree has to schedule tasks such that the intree contains two tasks that have as well scheduled as non-scheduled but ready predecessors. Figure \ref{fig:filling-op-is-not-strictly-optimal} shows the first steps of an optimal schedule for this intree.

\begin{figure}[th]
  \centering
  \includegraphics{p3/suboptimal/00022355666_optimal_no_fill_up.pdf}
  \caption{The optimal schedule for $(0,0,0,2,2,3,5,5,6,6,6)$ has two tasks (5 and 6) that have both scheduled and non-scheduled leaves as predecessors.}
  \label{fig:filling-op-is-not-strictly-optimal}
\end{figure}

\emph{Remark:} Surprisingly, every subtree of $(0,0,0,2,2,3,5,5,6,6,6)$ fulfills the conjecture that there is at most one task that has both scheduled and non-scheduled leaves as predecessors (during the whole schedule). This shows that even task that \emph{might seem} of minor impact for the first choices, might be relevant to the question which tasks are chosen in the optimal schedule.

\section{Maximizing 3-processor-time, minimizing 1-processor time}
\label{sec:p3-disproving-long-p3-and-short-p1-time}

Up to now, we maily focused on the structure of the current intree to derive strategies --- which all turned out to be (not strictly, but still) suboptimal. We now inspect another, more involved approach.

If we have three processors in total, we can split the total run time into three parts: The time where all three processors are processing tasks, the time where one processor is idle and two are working, and the time where only one processor is working.

%We first define some variants of run time. We consider the overall run time and the time where -- within a schedule of an intree -- exactly $p$ processors are working (i.e. where exactly $p$ tasks are scheduled).

\begin{definition}[Run time and its variants]
  We denote by $T$ the expected run time for a schedule associated with an intree. 
  Moreover, we denote the time where exactly $p$ taks are scheduled by $T_p$.
\end{definition}

Note that $T$ actually describes an \emph{expected value}. Because of the linearity of expectation, we have that -- for three processors -- $T=T_1 + T_2 + T_3$. If we want to construct an optimal schedule for three processors, we might be tempted to think that (at least) one of the two following criteria should be fulfilled for the optimal schedule:

\begin{description}
\item[P3L] For the optimal schedule, $T_3$ should be maximal (over all schedules), i.e. we should exploit three processors as long as possible (in the expectation).
\item[P1S] For the optimal schedule, $T_1$ should be minimal (over all schedules), i.e. we should try to keep the expected time for which only one processor is working as short as possible.
\end{description}

Surprisingly, \emph{both} of them are wrong (at least if considered separately).

\subsection{Comparing $T_1$, $T_2$ and $T_3$ for particular intrees}
\label{sec:comparing-t1-t2-t3-particular-intrees}

We compare the times where exactly one, two resp. three precessors are busy for selected intrees. In particular, we will compare the respective intrees that were investigated so far and whose optimal schedule is non-HLF. The results are shown in table \ref{tab:comparing-t1-t2-t3-selected-intrees}.

\begin{table}[th]
  \centering
  \begin{tabular}[ht]{l|ccc|ccc}
    \multirow{2}{*}{Intree} & \multicolumn{3}{c|}{(Suboptimal) HLF} & \multicolumn{3}{c}{Optimal schedule} \\
    & $T_3$ & $T_2$ & $T_1$ & $T_3^*$ & $T_2^*$ & $T_1^*$ \\
    \hline
    $(0,0,1,1,1,2)$ & 0.7778 & 1.0556 & 2.5556 & 0.8519 & 0.9259 & 2.5926 \\
    $(0,0,1,2,3,4,6,6,8,8)$ & 0.9630 & 2.1061 & 3.8989 & 1.1029 & 1.8267 & 4.0379 \\
    $(0,0,1,2,4,4,6,7,8,8)$ & 0.9698 & 1.4520 & 5.1865 & 1.0796 & 1.2329 & 5.2955 \\
    $(0,0,1,2,3,4,5,5,7,9,9)$ & 1.0288 & 2.1691 & 4.5754 & 1.1852 & 1.8628 & 4.7189 \\
    $(0,0,1,2,2,3,3,6,8,9,10,11,12)$ & 1.1231 & 1.7182 & 7.1942 & 1.2466 & 1.4713 & 7.3176 \\
    $(0,0,0,2,3,4,5,7,7,9,10,10)$ & 1.7905 & 1.6275 & 4.3736 & 1.8899 & 1.4323 & 4.4658 \\
    % the following needs some comment on "suboptimal hlf"
    $(0,0,0,1,1,1,2,2,3)$ & 2.0069 & 0.7843 & 2.4108 & 2.0137 & 0.7723 & 2.4143 \\ 
    $(0,0,1,1,2,3,3,4,5,7,8,9,9,11,(13)^3)$ 
      & 3.8694 & 1.6105 & 3.1709 & 3.8783 & 1.5927 & 3.1796 \\
    $(0,0,0,2,2,3,5,5,6,6,6)$ & 2.4127 & 1.0310 & 2.7000 & 2.4171 & 1.0224 & 2.7040
  \end{tabular}
  \caption{Comparing times where exactly one, two resp. three processors are busy.}
  \label{tab:comparing-t1-t2-t3-selected-intrees}
\end{table}

We observe -- for the intrees considered here -- that $T_3^* \geq T_3$ and (surprisingly) also $T_1^* \geq T_1$. As a consequence, we always have $T_2^* \leq T_2$. In the following, we will inspect some examples that show that this is not always the case.

\subsection{Maximizing $T_3$}
\label{sec:p3-disproving-long-p3}

Figure \ref{fig:p3-p3l-suboptimal-example} shows an example, where the optimal schedule keeps three processors busy for expected 0.77777 time steps, while a suboptimal schedule keeps three processors busy for a longer expected time, namely about 0.851852 time steps.

From this we can conclude that it may be advantageous in some cases to accept a shorter time with three busy processors, thereby possibly also decreasing the time where only one processor is busy.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/keep_3_busy/three_busy_opt.pdf}
    \caption{Optimal schedule. Keeps three processors busy for $7/9\approx 0.78$ time steps ($(T_3, T_2, T_1)=(7/9, 31/24, 37/12)$).}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/keep_3_busy/three_busy_subopt.pdf}
    \caption{This suboptimal schedule keeps three processors busy for expectedly $0.851852$ time steps ($(T_3, T_2, T_1)=(23/27,10/9,29/9)$).}
  \end{subfigure}
  \caption{An intree that shows that an optimal P3 schedule needs not keep busy three processors as long as possible. Snapshots with fewer than 6 tasks omitted since they have at most two tasks to be schedlued can be (optimally) processed via ordinary HLF.}
  \label{fig:p3-p3l-suboptimal-example}
\end{figure}

\subsection{Minimizing $T_1$}
\label{sec:p3-disproving-short-p1}

The ``other direction'', i.e. minimizing the time where only one processor is busy, still is suboptimal.
Figure \ref{fig:p3-p1s-suboptimal-example} shows an intree with the property that the optimal schedule has an expected timespan of roughly 2.59259, within which only one processor is busy. On the other hand, a suboptimal schedule has a timespan of roughly 2.55555 within which only one processor is busy.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/keep_1_unbusy/one_unbusy_opt.pdf}
    \caption{Optimal schedule. For expectedly $70/27\approx 2.59$ time steps, only one processor is busy $(T_3, T_2, T_1)=(23/27, 25/27, 70/27)$.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.45\linewidth}
    \centering
    \includegraphics{p3/keep_1_unbusy/one_unbusy_subopt.pdf}
    \caption{This suboptimal schedule has an approximated timespan of $23/9\approx 2.55$ time steps, where only one processor is working ($(T_3, T_2, T_1)=(7/9,19/18,23/9)$).}
  \end{subfigure}
  \caption{An intree where the expected time with only one processor being busy is longer within the optimal schedule ($\approx 2.59259$) than within a suboptimal schedule ($\approx 2.555555$).}
  \label{fig:p3-p1s-suboptimal-example}
\end{figure}

This shows that it can be useful to accept a longer time with only one processor busy, probably acchieving a longer time span where three processors are busy.

\subsection{Maximizing $T_3$ \emph{or} minimizing $T_1$}
\label{sec:p3-suboptimality-maximizing-t3-and-minimizing-t1}

It can also be shown that even combining the two arguments -- in the sense that P3L \emph{or} P1S should be fulfilled for the optimal schedule -- is not correct. This can be observed by examining the intree $(0, 0, 1, 1, 2, 3, 3, 3)$. Figure \ref{fig:p3l-p1s-combo-suboptimal} shows this example.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics{p3/max_p3_min_p1/00112333opt.pdf}
    \caption{Optimal schedule. $(T_3^*,T_2^*,T_1^*)=(\frac{4}{3},\frac{217}{216},\frac{323}{108})$.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics{p3/max_p3_min_p1/00112333t1min.pdf}
    \caption{Schedule with minimal $T_1$. $(T_3,T_2,T_1)=(\frac{290}{243},\frac{2369}{1944},\frac{2899}{972})$.}
  \end{subfigure}
  \quad
  \begin{subfigure}{.3\linewidth}
    \centering
    \includegraphics{p3/max_p3_min_p1/00112333t3max.pdf}
    \caption{Schedule with maximal $T_3$. $(T_3,T_2,T_1)=(\frac{110}{81},\frac{299}{324},\frac{499}{162})$.}
  \end{subfigure}
  \caption{A combination of P3L and P1S is not a criterion for an optimal schedule. The optimal schedule has $T_3^*=\frac{4}{3}\approx 1.333$ and $T_1^*=\frac{323}{108}\approx 2.99074$. One other (suboptimal) schedule has $T_1=\frac{2899}{972}\approx 2.98251 < T_1^*$, while still another schedule has $T_3=\frac{110}{81}\approx1.358025 > T_3^*$.}
  \label{fig:p3l-p1s-combo-suboptimal}
\end{figure}

\begin{corollary}
  Let $T^s$ denote the overall run time of a schedule $s$ and $T_1^s$, $T_2^s$ and $T_3^s$ be the times where exactly three, two and one tasks are scheduled within this schedule, respectively.

  Let $I$ be an intree and $S$ be the set of all schedules. Let $s^*$ be the optimal schedule, which has associated the optimal run time $T^*$, with $T_1^*, T_2^*, T_3^*$ being its parts.
  \begin{itemize}
  \item It may be the case that there is a schedule $s\in S$ such that $T_3^s \geq T_3^*$.
  \item It may be the case that there is a schedule $s\in S$ such that $T_1^s \leq T_1^*$.
  \end{itemize}
\end{corollary}

That is, it is not necessarily the case that $T_3$ is maximal for the optimal schedule, nor is it necessarily the case that $T_1$ is minimal for the optimal schedule.

However, after some investigation, we are tempted to conjecture the following.

\begin{conjecture}
  Let $I$, $T^s, T_1^s, T_2^s, T_3^s$ and $S$ be as defined above. Let $s^*$ be the optimal schedule for $I$ associated with the respective times $T_1^*, T_2^*, T_3^*$. Then, there is no schedule $s\in S$ such that
  \begin{equation*}
    T^s > T^* \wedge T_1^s \leq T_1^* \wedge T_3^s \geq T_3^*.
  \end{equation*}
\end{conjecture}

Even if this conjecture turns out to be true, it seems complex to transform this knowledge into a scheduling strategy that does something more significantly efficient than ``explore everything, and choose the best'', because $T_3, T_2$ and $T_1$ are not that easy to compute.

\section{Conclusion}
\label{sec:p3-conclusion}

Unfortunately, we did not find any strategy that always yields an optimal schedule. Of course, it is still possible to compute the optimal schedule by an exhaustive search.

During our research, we recognized some patterns that we are tempted to transform into conjectures. We were, however, not yet able to prove or disprove them. These conjectures might be used to reduce the number of snapshots that need to be examined by an exhaustive search used to compute the optimal snapshot.

This section shows the most important conjectures we found.

\begin{conjecture}
  \label{conj:as-many-topmost-as-possibly}
  An optimal schedule always schedules as many topmost tasks as possible.
\end{conjecture}

Please note that the above conjecture does not state anything about \emph{which} topmost tasks should be chosen in order to generate a schedule that is as good as possible. It can -- however -- drastically reduce the number of choices for the tasks to be scheduled. As an example, consider the following tree:

\begin{center}
  \begin{tikzpicture}[scale=.2]
    \node[circle, scale=0.75, fill] (tid0) at (7.5,1.5){};
    \node[circle, scale=0.75, fill] (tid1) at (0.75,3){};
    \node[circle, scale=0.75, fill] (tid2) at (5.25,3){};
    \node[circle, scale=0.75, fill] (tid4) at (2.25,4.5){};
    \node[circle, scale=0.75, fill] (tid5) at (3.75,4.5){};
    \node[circle, scale=0.75, fill] (tid8) at (3.75,6){};
    \draw[](tid5) -- (tid8);
    \node[circle, scale=0.75, fill] (tid6) at (6.75,4.5){};
    \node[circle, scale=0.75, fill] (tid9) at (6,6){};
    \node[circle, scale=0.75, fill] (tid13) at (5.25,7.5){};
    \node[circle, scale=0.75, fill] (tid14) at (6.75,7.5){};
    \draw[](tid9) -- (tid13);
    \draw[](tid9) -- (tid14);
    \node[circle, scale=0.75, fill] (tid10) at (8.25,6){};
    \node[circle, scale=0.75, fill] (tid15) at (8.25,7.5){};
    \draw[](tid10) -- (tid15);
    \draw[](tid6) -- (tid9);
    \draw[](tid6) -- (tid10);
    \draw[](tid2) -- (tid4);
    \draw[](tid2) -- (tid5);
    \draw[](tid2) -- (tid6);
    \node[circle, scale=0.75, fill] (tid3) at (12,3){};
    \node[circle, scale=0.75, fill] (tid7) at (12,4.5){};
    \node[circle, scale=0.75, fill] (tid11) at (9.75,6){};
    \node[circle, scale=0.75, fill] (tid12) at (12.75,6){};
    \node[circle, scale=0.75, fill] (tid16) at (11.25,7.5){};
    \node[circle, scale=0.75, fill] (tid17) at (12.75,7.5){};
    \node[circle, scale=0.75, fill] (tid18) at (14.25,7.5){};
    \draw[](tid12) -- (tid16);
    \draw[](tid12) -- (tid17);
    \draw[](tid12) -- (tid18);
    \draw[](tid7) -- (tid11);
    \draw[](tid7) -- (tid12);
    \draw[](tid3) -- (tid7);
    \draw[](tid0) -- (tid1);
    \draw[](tid0) -- (tid2);
    \draw[](tid0) -- (tid3);
  \end{tikzpicture}
\end{center}

This tree has 6 topmost tasks, but 10 leaves in total. If conjecture \ref{conj:as-many-topmost-as-possibly} is correct, then we can restrict ourselves to combinations of 6 topmost tasks -- being at most $\binom{6}{3}=20$ possible choices, in this particular case even only 6 due to equivalence of snapshots. In contrast, considering all 10 leaves, we have 48 possible choices in the above example.

Note that conjecture \ref{conj:as-many-topmost-as-possibly} also helps us to restrict the number of snapshots in another way: If we have to keep two tasks scheduled (because they were already scheduled in the previous step), we possibly do not need to examine all other tasks to be scheduled. If there are topmost tasks remaining, we can focus on them and do not need to examine non-topmost tasks.

Moreover, we are tempted to say the following:

\begin{conjecture}
  \label{conj:only-nontop-tasks-exchange-better}
  If for an intree only non-top tasks are scheduled, you can schedule any top-task instead of one non-top scheduled task to obtain a better run time.
\end{conjecture}

Conjecture \ref{conj:only-nontop-tasks-exchange-better} is not that useful in a direct application to reduce the number of snapshots needed to examine if we do an exhaustive search. However, it might be useful for proofs.

The main problems we faced when we tried to prove the above conjectures can be summarized as follows:
\begin{itemize}
\item If working with particular cases of intrees, the structure is not necessarily maintained over the induction step --- and if so, many case distinctions may be required.
\item Comparing different intrees seems to be quite cumbersome, especially if we do not know which tasks are scheduled.
\end{itemize}


%%% Local Variables:
%%% TeX-master: "../thesis.tex"
%%% End: 